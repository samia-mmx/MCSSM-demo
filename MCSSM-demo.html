<html>
  <head>

    <style>
    h1 {
      font-family: 'Open Sans', sans-serif;
      font-size: 30px;
      line-height:250%;
      font-weight: 500;
      text-rendering: optimizeLegibility;
    }
    h2 {
      font-family: 'Open Sans', sans-serif;
      letter-spacing: 1px;
      letter-spacing: -0.015em;
      font-weight: 300;
      line-height:150%;
      text-rendering: optimizeLegibility;
    }
    body {
        font-family: 'Open Sans', sans-serif;
        font: normal 12px/150% Arial, Helvetica, sans-serif;
        background: #fff;
    }
    table, td {
        border: 1px solid black;
        border-collapse: collapse;
        padding: 2px 2px;
        text-align: center;
    }
    th {
        padding: 3px 10px;
        font-family: 'Open Sans', sans-serif;
        font-weight: 100;
        background-color:#DADADA;
        color:#000000;
        font-size: 15px;
        border-left: 1px solid black;
        height: 30px;
    }
    table tr {
        color: #000000;
        border: 1px solid black;
        font-size: 12px;
        font-weight: normal;
        height: 40px;
    }
    audio {
      width: 150px;
      padding: 1px;
    }
    div {
      font-family: 'Open Sans', sans-serif;
      font-weight: 100;
      font-size: 15px;
      line-height: 24px;
    }
    </style>

    <meta charset="UTF-8">
    <title>Learning Language and Speaker Information for End-to-End Code-Switch Speech Synthesis</title>
  </head>

  <body>
    <article>
      <header>
        <h1>Learning Language and Speaker Information for End-to-End Code-Switch Speech Synthesis</h1>
      </header>
    </article>
    <!-- <br> -->
    <!-- <div style="font-size: 20px;"><b>Paper:</b> <a href="https://arxiv.org/">arXiv</a> </div> -->
      <div><b>Authors:</b> Shaotong Guo , Mengxin Chai , Cheng Gong , Longbiao Wang , Jianwu Dang , Ju Zhang</div>
<!--     <div><b>Abstract:</b> State-of-the-art text-to-speech (TTS) synthesis models can produce monolingual speech with high intelligibility and naturalness. However, when the models are applied to synthesize code-switched (CS) speech, the performance declines seriously. Conventionally, developing a CS TTS system requires multilingual data to incorporate language-specific and cross-lingual knowledge. Recently, end-to-end (E2E) architecture has achieved satisfactory results in monolingual TTS. The architecture enables the training from one end of alphabetic text input to the other end of acoustic feature output. In this paper, we explore the use of E2E framework for CS TTS, using a combination of Mandarin and English monolingual speech corpus uttered by two female speakers. To handle alphabetic input from different languages, we explore two kinds of encoders: (1) shared multilingual encoder with explicit language embedding (LDE); (2) separated monolingual encoder (SPE) for each language. The two systems use identical decoder architecture, where a discriminative code is incorporated to enable the model to generate speech in one speaker's voice consistently. Experiments confirm the effectiveness of the proposed modifications on the E2E TTS framework in terms of quality and speaker similarity of the generated speech. Moreover, our proposed systems can generate controllable foreign-accented speech at character-level using only mixture of monolingual training data. </div> -->
  <div><b>Abstract:</b> In this paper, we propose the use of language and speaker information and only use a small mix-lingual data to realize a CS synthesis of Mandarin and English. In order to distinguish the different languages in the input text, we explore two kinds of representation of text: (1) using different kinds of characters to represent the two languages (implicit); (2) using masks to separate
different languages (explicit). What’s more, Our model having a speaker extraction module. The learned speaker information and traditional character features are fed into the decoder simultaneously for generating the final Mel spectrogram. We conduct
a series of experiments on speaker consistency and the naturalness of speech.</div>
  <div><b>Notes:</b> In order to obtain best quality, we strongly encourage the listeners to take their headphones.</div>

    <div>

    <h3>Definition (same as defined in our submitted paper)</h3>
<!--   <td> <strong>• single speaker CS (SCS): </strong>just have a text encoder.</td><br>  -->
    <td> <strong>• muti-speaker CS with speaker embedding and mask embedding (MCSSM): </strong><br>
        with language marker encoder, text encoder and speaker feature encoder.</td><br>
    <td> <strong>• muti-speaker CS with speaker embedding (MCSS): </strong>
        with text encoder and speaker feature encoder.</td><br>
  <td> <strong>• muti-speaker CS with mask embedding (MCSM): </strong>
        with language marker encoder, text encoder and speaker feature encoder. </td><br>

    </div>

<br>

  <!--
    <br>

    <div style="font-size: 20px;"><b>Authors:</b> Cheng Gong, Longbiao Wang, Zhenhua Ling, Shaotong Guo, Ju Zhang, Jianwu Dang</div>
    <br>
    <div style="font-size: 20px; width: 1200px;"><b>Abstract:</b>State-of-the-art neural text-to-speech (TTS) networks are trained with a large amount of speech data, enabling the generation of speech that can be indistinguishable from natural speech. However, the prosody and controllability of the generated speech is still insufficient, especially in non English languages. Moreover, the generated prosody is solely defined by the input text, which does not allow for different styles for the same sentence or words. In this paper, we extend Tacotron2 with a pitch prediction task to capture pitch-related representations. Specifically, the learned pitch related suprasegmental information simulltaneously along with traditional characters features will be fed into decoder to
generate final Mel spectrogram. Experiments show that the proposed method can improve the quality of the generated speech (4.37 VS 4.22 in MOS ). We also demonstrate that we can easily achieve word-level pitch-accent control during generation by changing pitch-related representations before passing it to the decoder network.</div>

    <br>
    -->
    <!--
    <br>
    <div style="font-size: 18px"><h2>The following samples demonstrate the prosody control capability by adjusting bias of each prosodic dimension.</div>
    <br>
    <div><h2>Baseline: -</div>
    <table>
      <thead>
        <tr>
          <th>[No control]</th><th style="background-color: #f2f2f2;">-</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th style="background-color: #f2f2f2;">-</th>
          <td><audio controls=""><source src="audio/syn_baseline/syn_137.wav" type="audio/wav"></audio></td>
        </tr>
      </tbody>
    </table>
      <h3>Definition (same as defined in  our submitted paper)</h3>

    <br>
    <br>

    <div>
     <h3>Definition (same as defined in  our submitted paper)</h3>
     <strong>Baseline</strong>: Baseline model trained without pitch encoder and pitch-related representations.<br>
     <strong>Proposed_NoVQ</strong>: Our proposed model was trained without the VQ codebook, such that the pitch-related representations concatenated with the linguistic encoder output was continuous.<br>
     <strong>Proposed_VQ</strong>: Our proposed model was trained with the VQ codebook, such that the pitch-related representations concatenated with the linguistic encoder output were discrete.
      </div>
 -->
    <div><h2> <b>Speaker Consistency:</b> Evaluate whether it is the same speaker</div>
    <table>
      <thead>
        <tr>
          <th>Speaker</th>
          <th style="background-color: #b3d1ff;">speaker 1</th>
          <th style="background-color: #e6f0ff;">speaker 2</th>
          <th style="background-color: #ffebe6;">speaker 3</th>
        </tr>
      </thead>
      <tbody>
            <tr>
          <th style="background-color: #f2f2f2;">Real Audio</th>
          <td><audio controls=""><source src="hxt_000170.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="004_zjk_006435.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="006_000002.wav" type="audio/wav"></audio></td>
        </tr>
        <tr>
          <th style="background-color: #f2f2f2;">MCSSM</th>
          <td><audio controls=""><source src="mcsse_hxt_speaker1_18beat.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="mcsse_zjk_speaker2_18.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="mcsse_006_speaker1_18.wav.wav" type="audio/wav"></audio></td>
        </tr>
        <!--<tr>
          <th style="background-color: #f2f2f2;">Proposed_NoVQ</th>
          <td><audio controls=""><source src="audio1/text1_NO_1.4.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="audio1/text1_NO_1.2.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="audio1/text1_NO_1.0.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="audio1/text1_NO_0.8.wav" type="audio/wav"></audio></td>
           <td><audio controls=""><source src="audio1/text1_NO_0.6.wav" type="audio/wav"></audio></td>
        </tr>
        <tr>
          <th style="background-color: #f2f2f2;">Proposed_VQ</th>
           <td><audio controls=""><source src="audio1/text1_VQ_1.4.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="audio1/text1_VQ_1.2.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="audio1/text1_VQ_1.0.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="audio1/text1_VQ_0.8.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="audio1/text1_VQ_0.6.wav" type="audio/wav"></audio></td>
        </tr>
        -->
      </tbody>
    </table>
    <br>
    <div><h2>Text: 我们只会用一整天的时间<b>debug</b>代码。</div>
<table>
      <thead>
        <tr>
          <th>Method</th>
          <th style="background-color: #b3d1ff;">MCSSM</th>
          <th style="background-color: #e6f0ff;">MCSM</th>
          <th style="background-color: #ffffff;">MCSS</th>

        </tr>
      </thead>
      <tbody>
            <tr>
          <th style="background-color: #f2f2f2;">speaker 1</th>
          <td><audio controls=""><source src="mcsse_hxt_speaker1_6.wav" type="audio/wav"></audio></td>
          <td>-</td>
          <td><audio controls=""><source src="mcss_hxt_speaker1_6.wav" type="audio/wav"></audio></td>

        </tr>
        <tr>
          <th style="background-color: #f2f2f2;">speaker 2</th>
          <td><audio controls=""><source src="mcsse_zjk_speaker2_6.wav" type="audio/wav"></audio></td>
          <td>-</audio></td>
          <td><audio controls=""><source src="mcss_zjk_speaker2_6.wav" type="audio/wav"></audio></td>

        </tr>
        <tr>
          <th style="background-color: #f2f2f2;">speaker 3</th>
          <td><audio controls=""><source src="mcsse_006_speaker3_6.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="mcse_006_speaker3_6debug.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="mcss_006_speaker3_6.wav" type="audio/wav"></audio></td>
        </tr>

      </tbody>
    </table>
      <br>
      <br>
  </body>
</html>
